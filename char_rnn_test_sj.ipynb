{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "\n",
    "import collections\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load Hangulpy/Hangulpy.py\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\"\"\"\n",
    "Hangulpy.py\n",
    "\n",
    "Copyright (C) 2012 Ryan Rho, Hyunwoo Cho\n",
    "Text Decompose & Automata Extention by bluedisk@gmail\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n",
    "of the Software, and to permit persons to whom the Software is furnished to do\n",
    "so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "\n",
    "################################################################################\n",
    "# Hangul Unicode Variables\n",
    "################################################################################\n",
    "\n",
    "# Code = 0xAC00 + (Chosung_index * NUM_JOONGSUNGS * NUM_JONGSUNGS) + (Joongsung_index * NUM_JONGSUNGS) + (Jongsung_index)\n",
    "CHOSUNGS = [u'ㄱ',u'ㄲ',u'ㄴ',u'ㄷ',u'ㄸ',u'ㄹ',u'ㅁ',u'ㅂ',u'ㅃ',u'ㅅ',u'ㅆ',u'ㅇ',u'ㅈ',u'ㅉ',u'ㅊ',u'ㅋ',u'ㅌ',u'ㅍ',u'ㅎ']\n",
    "JOONGSUNGS = [u'ㅏ',u'ㅐ',u'ㅑ',u'ㅒ',u'ㅓ',u'ㅔ',u'ㅕ',u'ㅖ',u'ㅗ',u'ㅘ',u'ㅙ',u'ㅚ',u'ㅛ',u'ㅜ',u'ㅝ',u'ㅞ',u'ㅟ',u'ㅠ',u'ㅡ',u'ㅢ',u'ㅣ']\n",
    "JONGSUNGS = [u'',u'ㄱ',u'ㄲ',u'ㄳ',u'ㄴ',u'ㄵ',u'ㄶ',u'ㄷ',u'ㄹ',u'ㄺ',u'ㄻ',u'ㄼ',u'ㄽ',u'ㄾ',u'ㄿ',u'ㅀ',u'ㅁ',u'ㅂ',u'ㅄ',u'ㅅ',u'ㅆ',u'ㅇ',u'ㅈ',u'ㅊ',u'ㅋ',u'ㅌ',u'ㅍ',u'ㅎ']\n",
    "\n",
    "# 코딩 효율과 가독성을 위해서 index대신 unicode사용 by bluedisk\n",
    "JONG_COMP = {\n",
    "    u'ㄱ':{\n",
    "        u'ㄱ': u'ㄲ',\n",
    "        u'ㅅ': u'ㄳ',\n",
    "    },\n",
    "    u'ㄴ':{\n",
    "        u'ㅈ': u'ㄵ',\n",
    "        u'ㅎ': u'ㄶ',\n",
    "    },\n",
    "    u'ㄹ':{\n",
    "        u'ㄱ': u'ㄺ',\n",
    "        u'ㅁ': u'ㄻ',\n",
    "        u'ㅂ': u'ㄼ',\n",
    "        u'ㅅ': u'ㄽ',\n",
    "        u'ㅌ': u'ㄾ',\n",
    "        u'ㅍ': u'ㄿ',\n",
    "        u'ㅎ': u'ㅀ',\n",
    "    }\n",
    "}\n",
    "\n",
    "NUM_CHOSUNGS = 19\n",
    "NUM_JOONGSUNGS = 21\n",
    "NUM_JONGSUNGS = 28\n",
    "\n",
    "FIRST_HANGUL_UNICODE = 0xAC00 #'가'\n",
    "LAST_HANGUL_UNICODE = 0xD7A3 #'힣'\n",
    "\n",
    "# 한자와 라틴 문자 범위 by bluedisk\n",
    "FIRST_HANJA_UNICODE = 0x4E00\n",
    "LAST_HANJA_UNICODE = 0x9FFF\n",
    "\n",
    "FIRST_HANJA_EXT_A_UNICODE = 0x3400\n",
    "LAST_HANJA_EXT_A_UNICODE = 0x4DBF\n",
    "\n",
    "FIRST_LATIN1_UNICODE = 0x0000 # NUL\n",
    "LAST_LATIN1_UNICODE = 0x00FF # 'ÿ'\n",
    "\n",
    "# EXT B~E 는 무시\n",
    "\n",
    "################################################################################\n",
    "# Hangul Automata functions by bluedisk@gmail.com\n",
    "################################################################################\n",
    "COMPOSE_CODE = u'ᴥ'\n",
    "\n",
    "def decompose_text(text, latin_filter=True):\n",
    "    result=u\"\"\n",
    "\n",
    "    for c in list(text):\n",
    "        if is_hangul(c):\n",
    "\n",
    "            result = result + \"\".join(decompose(c)) + COMPOSE_CODE\n",
    "\n",
    "        else:\n",
    "            if latin_filter:    # 한글 외엔 Latin1 범위까지만 포함 (한글+영어)\n",
    "                if is_latin1(c):\n",
    "                    result = result + c\n",
    "            else:\n",
    "                result = result + c\n",
    "\n",
    "    return result\n",
    "\n",
    "def automata(text):\n",
    "    res_text = u\"\"\n",
    "    status=\"CHO\"\n",
    "\n",
    "    for c in text:\n",
    "\n",
    "        if status == \"CHO\":\n",
    "\n",
    "            if c in CHOSUNGS:\n",
    "                chosung = c\n",
    "                status=\"JOONG\"\n",
    "            else:\n",
    "                if c != COMPOSE_CODE:\n",
    "\n",
    "                    res_text = res_text + c\n",
    "\n",
    "        elif status == \"JOONG\":\n",
    "\n",
    "            if c != COMPOSE_CODE and c in JOONGSUNGS:\n",
    "                joongsung = c\n",
    "                status=\"JONG1\"\n",
    "            else:\n",
    "                res_text = res_text + chosung\n",
    "\n",
    "                if c in CHOSUNGS:\n",
    "                    chosung = c\n",
    "                    status=\"JOONG\"\n",
    "                else:\n",
    "                    if c != COMPOSE_CODE:\n",
    "\n",
    "                        res_text = res_text + c\n",
    "                    status=\"CHO\"\n",
    "\n",
    "        elif status == \"JONG1\":\n",
    "\n",
    "            if c != COMPOSE_CODE and c in JONGSUNGS:\n",
    "                jongsung = c\n",
    "\n",
    "                if c in JONG_COMP:\n",
    "                    status=\"JONG2\"\n",
    "                else:\n",
    "                    res_text = res_text + compose(chosung, joongsung, jongsung)\n",
    "                    status=\"CHO\"\n",
    "\n",
    "            else:\n",
    "                res_text = res_text + compose(chosung, joongsung)\n",
    "\n",
    "                if c in CHOSUNGS:\n",
    "                    chosung = c\n",
    "                    status=\"JOONG\"\n",
    "                else:\n",
    "                    if c != COMPOSE_CODE:\n",
    "\n",
    "                        res_text = res_text + c\n",
    "\n",
    "                    status=\"CHO\"\n",
    "\n",
    "        elif status == \"JONG2\":\n",
    "\n",
    "            if c != COMPOSE_CODE and c in JONG_COMP[jongsung]:\n",
    "                jongsung = JONG_COMP[jongsung][c]\n",
    "                c = COMPOSE_CODE # 종성 재 출력 방지\n",
    "\n",
    "            res_text = res_text + compose(chosung, joongsung, jongsung)\n",
    "\n",
    "            if c != COMPOSE_CODE:\n",
    "\n",
    "                res_text = res_text + c\n",
    "\n",
    "            status=\"CHO\"\n",
    "\n",
    "\n",
    "    return res_text\n",
    "\n",
    "################################################################################\n",
    "# Boolean Hangul functions\n",
    "################################################################################\n",
    "\n",
    "def is_hangul(phrase):\n",
    "    \"\"\"Check whether the phrase is Hangul.\n",
    "    This method ignores white spaces, punctuations, and numbers.\n",
    "    @param phrase a target string\n",
    "    @return True if the phrase is Hangul. False otherwise.\"\"\"\n",
    "\n",
    "    # If the input is only one character, test whether the character is Hangul.\n",
    "    if len(phrase) == 1: return is_all_hangul(phrase)\n",
    "\n",
    "    # Remove all white spaces, punctuations, numbers.\n",
    "    exclude = set(string.whitespace + string.punctuation + '0123456789')\n",
    "    phrase = ''.join(ch for ch in phrase if ch not in exclude)\n",
    "\n",
    "    return is_all_hangul(phrase)\n",
    "\n",
    "def is_all_hangul(phrase):\n",
    "    \"\"\"Check whether the phrase contains all Hangul letters\n",
    "    @param phrase a target string\n",
    "    @return True if the phrase only consists of Hangul. False otherwise.\"\"\"\n",
    "\n",
    "    for unicode_value in map(lambda letter:ord(letter), phrase):\n",
    "        if unicode_value < FIRST_HANGUL_UNICODE or unicode_value > LAST_HANGUL_UNICODE:\n",
    "            # Check whether the letter is chosungs, joongsungs, or jongsungs.\n",
    "            if unicode_value not in map(lambda v: ord(v), CHOSUNGS + JOONGSUNGS + JONGSUNGS[1:]):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def is_hanja(phrase):\n",
    "    for unicode_value in map(lambda letter:ord(letter), phrase):\n",
    "        if ((unicode_value < FIRST_HANJA_UNICODE or unicode_value > LAST_HANJA_UNICODE) and\n",
    "            (unicode_value < FIRST_HANJA_EXT_A_UNICODE or unicode_value > LAST_HANJA_EXT_A_UNICODE)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_latin1(phrase):\n",
    "\n",
    "    for unicode_value in map(lambda letter:ord(letter), phrase):\n",
    "        if unicode_value < FIRST_LATIN1_UNICODE or unicode_value > LAST_LATIN1_UNICODE:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def has_jongsung(letter):\n",
    "    \"\"\"Check whether this letter contains Jongsung\"\"\"\n",
    "    if len(letter) != 1:\n",
    "        raise Exception('The target string must be one letter.')\n",
    "    if not is_hangul(letter):\n",
    "        raise NotHangulException('The target string must be Hangul')\n",
    "\n",
    "    unicode_value = ord(letter)\n",
    "    return (unicode_value - FIRST_HANGUL_UNICODE) % NUM_JONGSUNGS > 0\n",
    "\n",
    "def has_batchim(letter):\n",
    "    \"\"\"This method is the same as has_jongsung()\"\"\"\n",
    "    return has_jongsung(letter)\n",
    "\n",
    "def has_approximant(letter):\n",
    "    \"\"\"Approximant makes complex vowels, such as ones starting with y or w.\n",
    "    In Korean there is a unique approximant euㅡ making uiㅢ, but ㅢ does not make many irregularities.\"\"\"\n",
    "    if len(letter) != 1:\n",
    "        raise Exception('The target string must be one letter.')\n",
    "    if not is_hangul(letter):\n",
    "        raise NotHangulException('The target string must be Hangul')\n",
    "\n",
    "    jaso = decompose(letter)\n",
    "    diphthong = (2, 3, 6, 7, 9, 10, 12, 14, 15, 17)\n",
    "    # [u'ㅑ',u'ㅒ',',u'ㅕ',u'ㅖ',u'ㅘ',u'ㅙ',u'ㅛ',u'ㅝ',u'ㅞ',u'ㅠ']\n",
    "    # excluded 'ㅢ' because y- and w-based complex vowels are irregular.\n",
    "    # vowels with umlauts (ㅐ, ㅔ, ㅚ, ㅟ) are not considered complex vowels.\n",
    "    return jaso[1] in diphthong\n",
    "\n",
    "################################################################################\n",
    "# Decomposition & Combination\n",
    "################################################################################\n",
    "\n",
    "def compose(chosung, joongsung, jongsung=u''):\n",
    "    \"\"\"This function returns a Hangul letter by composing the specified chosung, joongsung, and jongsung.\n",
    "    @param chosung\n",
    "    @param joongsung\n",
    "    @param jongsung the terminal Hangul letter. This is optional if you do not need a jongsung.\"\"\"\n",
    "\n",
    "    if jongsung is None: jongsung = u''\n",
    "\n",
    "    try:\n",
    "        chosung_index = CHOSUNGS.index(chosung)\n",
    "        joongsung_index = JOONGSUNGS.index(joongsung)\n",
    "        jongsung_index = JONGSUNGS.index(jongsung)\n",
    "    except Exception, e:\n",
    "        raise NotHangulException('No valid Hangul character can be generated using given combination of chosung, joongsung, and jongsung.')\n",
    "\n",
    "    return unichr(0xAC00 + chosung_index * NUM_JOONGSUNGS * NUM_JONGSUNGS + joongsung_index * NUM_JONGSUNGS + jongsung_index)\n",
    "\n",
    "def decompose(hangul_letter):\n",
    "    \"\"\"This function returns letters by decomposing the specified Hangul letter.\"\"\"\n",
    "\n",
    "    if len(hangul_letter) < 1:\n",
    "        raise NotLetterException('')\n",
    "    elif not is_hangul(hangul_letter):\n",
    "        raise NotHangulException('')\n",
    "\n",
    "    code = ord(hangul_letter) - FIRST_HANGUL_UNICODE\n",
    "    jongsung_index = code % NUM_JONGSUNGS\n",
    "    code /= NUM_JONGSUNGS\n",
    "    joongsung_index = code % NUM_JOONGSUNGS\n",
    "    code /= NUM_JOONGSUNGS\n",
    "    chosung_index = code\n",
    "\n",
    "    if chosung_index < 0:\n",
    "        chosung_index = 0\n",
    "\n",
    "    try:\n",
    "        return (CHOSUNGS[chosung_index], JOONGSUNGS[joongsung_index], JONGSUNGS[jongsung_index])\n",
    "    except:\n",
    "        print \"%d / %d  / %d\"%(chosung_index, joongsung_index, jongsung_index)\n",
    "        print \"%s / %s \" %( (JOONGSUNGS[joongsung_index].encode(\"utf8\"), JONGSUNGS[jongsung_index].encode('utf8')))\n",
    "        raise Exception()\n",
    "\n",
    "################################################################################\n",
    "# Josa functions\n",
    "################################################################################\n",
    "\n",
    "def josa_en(word):\n",
    "    \"\"\"add josa either '은' or '는' at the end of this word\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[-1]\n",
    "    josa = u'은' if has_jongsung(last_letter) else u'는'\n",
    "    return word + josa\n",
    "\n",
    "def josa_eg(word):\n",
    "    \"\"\"add josa either '이' or '가' at the end of this word\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[-1]\n",
    "    josa = u'이' if has_jongsung(last_letter) else u'가'\n",
    "    return word + josa\n",
    "\n",
    "def josa_el(word):\n",
    "    \"\"\"add josa either '을' or '를' at the end of this word\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[-1]\n",
    "    josa = u'을' if has_jongsung(last_letter) else u'를'\n",
    "    return word + josa\n",
    "\n",
    "def josa_ro(word):\n",
    "    \"\"\"add josa either '으로' or '로' at the end of this word\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[-1]\n",
    "    if not has_jongsung(last_letter):\n",
    "        josa = u'로'\n",
    "    elif (ord(last_letter) - FIRST_HANGUL_UNICODE) % NUM_JONGSUNGS == 9: # ㄹ\n",
    "        josa = u'로'\n",
    "    else:\n",
    "        josa = u'으로'\n",
    "\n",
    "    return word + josa\n",
    "\n",
    "def josa_gwa(word):\n",
    "    \"\"\"add josa either '과' or '와' at the end of this word\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[-1]\n",
    "    josa = u'과' if has_jongsung(last_letter) else u'와'\n",
    "    return word + josa\n",
    "\n",
    "def josa_ida(word):\n",
    "    \"\"\"add josa either '이다' or '다' at the end of this word\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[-1]\n",
    "    josa = u'이다' if has_jongsung(last_letter) else u'다'\n",
    "    return word + josa\n",
    "\n",
    "################################################################################\n",
    "# Prefixes and suffixes\n",
    "# Practice area; need more organization\n",
    "################################################################################\n",
    "\n",
    "def add_ryul(word):\n",
    "    \"\"\"add suffix either '률' or '율' at the end of this word\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[-1]\n",
    "    if not has_jongsung(last_letter):\n",
    "        ryul = u'율'\n",
    "    elif (ord(last_letter) - FIRST_HANGUL_UNICODE) % NUM_JONGSUNGS == 4: # ㄴ\n",
    "        ryul = u'율'\n",
    "    else:\n",
    "        ryul = u'률'\n",
    "\n",
    "    return word + ryul\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# The formatter, or ultimately, a template system\n",
    "# Practice area; need more organization\n",
    "################################################################################\n",
    "\n",
    "def ili(word):\n",
    "    \"\"\"convert {가} or {이} to their correct respective particles automagically.\"\"\"\n",
    "    word = word.strip()\n",
    "    if not is_hangul(word): raise NotHangulException('')\n",
    "\n",
    "    last_letter = word[word.find(u'{가}')-1]\n",
    "    word = word.replace(u'{가}', (u'이' if has_jongsung(last_letter) else u'가'))\n",
    "\n",
    "    last_letter = word[word.find(u'{이}')-1]\n",
    "    word = word.replace(u'{이}', (u'이' if has_jongsung(last_letter) else u'가'))\n",
    "    return word\n",
    "\n",
    "################################################################################\n",
    "# Exceptions\n",
    "################################################################################\n",
    "\n",
    "class NotHangulException(Exception):\n",
    "    pass\n",
    "\n",
    "class NotLetterException(Exception):\n",
    "    pass\n",
    "\n",
    "class NotWordException(Exception):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is for loading TEXT!\n",
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load text \n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_dir    = \"data/han1\"\n",
    "#data_dir    = \"data/han2\"\n",
    "#data_dir    = \"data/han3\"\n",
    "#data_dir    = \"data/han4\"\n",
    "#data_dir    = \"data/han5\"\n",
    "\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "# Define Network \n",
    "rnn_size   = 128\n",
    "num_layers = 2\n",
    "grad_clip  = 5.\n",
    "\n",
    "_batch_size = 1\n",
    "_seq_length = 1\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "# Select RNN Cell\n",
    "unitcell = rnn_cell.BasicLSTMCell(rnn_size)\n",
    "cell = rnn_cell.MultiRNNCell([unitcell] * num_layers)\n",
    "\n",
    "# Set paths to the graph \n",
    "input_data = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "targets    = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "initial_state = cell.zero_state(_batch_size, tf.float32)\n",
    "\n",
    "# Set Network\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "        inputs = tf.split(1, _seq_length, tf.nn.embedding_lookup(embedding, input_data))\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "# Loop function for seq2seq\n",
    "def loop(prev, _):\n",
    "    prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "# Output of RNN \n",
    "outputs, last_state = seq2seq.rnn_decoder(inputs, initial_state, cell, loop_function=None, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "# Next word probability \n",
    "probs = tf.nn.softmax(logits)\n",
    "# Define LOSS\n",
    "loss = seq2seq.sequence_loss_by_example([logits], # Input\n",
    "    [tf.reshape(targets, [-1])], # Target\n",
    "    [tf.ones([_batch_size * _seq_length])], # Weight \n",
    "    vocab_size)\n",
    "# Define Optimizer\n",
    "cost = tf.reduce_sum(loss) / _batch_size / _seq_length\n",
    "final_state = last_state\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "_optm = tf.train.AdamOptimizer(lr)\n",
    "optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sampling function \n",
    "def sample( sess, chars, vocab, __probs, num=200, prime=u'ㅇㅗᴥㄴㅡㄹᴥ '):\n",
    "    # state = cell.zero_state(1, tf.float32).eval()\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    _probs = __probs\n",
    "    \n",
    "    prime = list(prime)\n",
    "\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [_probsval, state] = sess.run([_probs, final_state], feed)\n",
    "        p = _probsval[0]\n",
    "        # sample = int(np.random.choice(len(p), p=p))\n",
    "        sample = weighted_pick(p)\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime Text : 오늘  => ㅇㅗᴥㄴㅡㄹᴥ \n",
      "(u'\\u1d25', u' ', u'\\u3147', u'\\u314f', u'\\u3134', u'\\u3131', u'\\u3139', u'\\u3163', u'\\u3161', u'\\u3153', u'\\u3137', u'\\u3157', u'\\u3145', u'\\u3141', u'\\u3148', u'\\u314e', u'\\u315c', u'\\u3142', u'\\n', u'\\r', u'\\u3155', u'\\u3154', u'\\u3150', u'\\u3146', u'.', u'\\u314a', u'\\u3162', u'\\u3158', u'\"', u'\\u314c', u'\\u314d', u'\\u3132', u'\\u315b', u',', u'\\u3138', u'\\u3151', u'\\u315a', u'\\u314b', u'\\u315f', u'\\u3136', u'\\u315d', u'\\u3144', u'?', u'\\u3149', u'\\u3160', u'-', u'\\u3156', u'\\u3143', u\"'\", u')', u'(', u'!', u'=', u'\\u3159', u'1', u'\\u313a', u'0', u'+', u'2', u'[', u']', u'e', u'\\u3140', u'\\u3135', u'3', u'a', u'\\u313b', u'o', u't', u':', u'>', u'<', u'5', u'i', u'r', u'n', u'4', u'9', u'\\u3152', u'6', u's', u'8', u'\\u313c', u'7', u'd', u'l', u'\\u315e', u'h', u'/', u'c', u'g', u'u', u'm', u'*', u'b', u'p', u'y', u'S', u'A', u';', u'f', u'^', u'T', u'w', u'I', u'k', u'C', u'F', u'D', u'~', u'\\t', u'L', u'B', u'M', u'v', u'N', u'\\u3133', u'E', u'H', u'P', u'\\u313e', u'O', u'`', u'K', u'_', u'R', u'W', u'#', u'Y', u'G', u'\\xb7', u'&', u'V', u'%', u'x', u'j', u'J', u'z', u'\\u313f', u'X', u'U', u'\\u313d', u'@', u'Z', u'q', u'}', u'{', u'\\x0c', u'Q', u'\\x01', u'$', u'\\x1a', u'\\x16', u'|', u'\\xd7', u'\\x06', u'\\\\', u'\\xa8', u'\\x17', u'\\x03', u'\\x10', u'\\x1b', u'\\x19', u'\\x18', u'\\x04', u'\\xf7', u'\\x0f', u'\\x02', u'\\xb1', u'\\xbd', u'\\xa4', u'\\xb0', u'\\xd8', u'\\xf8', u'\\x0e', u'\\xa1', u'\\x15', u'\\xb2')\n",
      "{u'\\x04': 164, u'\\x0c': 147, u'\\x10': 160, u'\\x18': 163, u' ': 1, u'$': 150, u'(': 50, u',': 33, u'0': 56, u'\\u3132': 31, u'4': 76, u'\\xb7': 130, u'\\u3136': 39, u'8': 81, u'\\u313a': 55, u'<': 71, u'\\u313e': 120, u'@': 142, u'\\u3142': 17, u'D': 108, u'\\u3146': 23, u'H': 118, u'\\u314a': 25, u'L': 111, u'\\u314e': 15, u'P': 119, u'\\u3152': 78, u'T': 102, u'\\xd7': 154, u'\\u3156': 46, u'X': 139, u'\\u315a': 36, u'\\\\': 156, u'\\u315e': 86, u'`': 122, u'\\u3162': 26, u'd': 84, u'h': 87, u'l': 85, u'p': 95, u't': 68, u'\\xf7': 165, u'x': 134, u'|': 153, u'\\x03': 159, u'\\x0f': 166, u'\\x17': 158, u'\\x1b': 161, u'#': 127, u'\\u1d25': 0, u'\\xa4': 170, u\"'\": 48, u'\\xa8': 157, u'+': 57, u'/': 88, u'\\u3131': 5, u'\\xb0': 171, u'3': 64, u'\\u3135': 63, u'7': 83, u'\\u3139': 6, u';': 99, u'\\u313d': 141, u'?': 42, u'\\u3141': 13, u'C': 106, u'\\u3145': 12, u'G': 129, u'\\u3149': 43, u'K': 123, u'\\u314d': 30, u'O': 121, u'\\u3151': 35, u'S': 97, u'\\u3155': 20, u'W': 126, u'\\u3159': 53, u'\\xd8': 172, u'[': 59, u'\\u315d': 40, u'_': 124, u'\\u3161': 8, u'c': 89, u'g': 90, u'k': 105, u'o': 67, u's': 80, u'w': 103, u'\\xf8': 173, u'{': 146, u'\\x02': 167, u'\\x06': 155, u'\\n': 18, u'\\x0e': 174, u'\\x16': 152, u'\\x1a': 151, u'\\xa1': 175, u'\"': 28, u'&': 131, u'*': 93, u'.': 24, u'\\xb1': 168, u'2': 58, u'\\u3134': 4, u'6': 79, u'\\u3138': 34, u':': 69, u'\\xbd': 169, u'\\u313c': 82, u'>': 70, u'\\u3140': 62, u'B': 112, u'\\u3144': 41, u'F': 107, u'\\u3148': 14, u'J': 136, u'\\u314c': 29, u'N': 115, u'\\u3150': 22, u'R': 125, u'\\u3154': 21, u'V': 132, u'\\u3158': 27, u'Z': 143, u'\\u315c': 16, u'^': 101, u'\\u3160': 44, u'b': 94, u'f': 100, u'j': 135, u'n': 75, u'r': 74, u'v': 114, u'z': 137, u'~': 109, u'\\x01': 149, u'\\t': 110, u'\\r': 19, u'\\x15': 176, u'\\x19': 162, u'!': 51, u'%': 133, u')': 49, u'-': 45, u'1': 54, u'\\u3133': 116, u'\\xb2': 177, u'5': 72, u'\\u3137': 10, u'9': 77, u'\\u313b': 66, u'=': 52, u'\\u313f': 138, u'A': 98, u'\\u3143': 47, u'E': 117, u'\\u3147': 2, u'I': 104, u'\\u314b': 37, u'M': 113, u'\\u314f': 3, u'Q': 148, u'\\u3153': 9, u'U': 140, u'\\u3157': 11, u'Y': 128, u'\\u315b': 32, u']': 60, u'\\u315f': 38, u'a': 65, u'\\u3163': 7, u'e': 61, u'i': 73, u'm': 92, u'q': 144, u'u': 91, u'y': 96, u'}': 145}\n",
      "save/model.ckpt-1000\n",
      "\n",
      "SAMPLED TEXT = [u'\\u3147', u'\\u3157', u'\\u1d25', u'\\u3134', u'\\u3161', u'\\u3139', u'\\u1d25', u' ', u'\\u3148', u'\\u3131', u'\\u3137', u'\\u1d25', u'\\u314f', u'\\u3142', u'\\u3147', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u3161', u'\\u3154', u'\\u1d25', u'\\u3143', u'\\u3158', u'\\u314f', u'\\u3131', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u3134', u'\\u3153', u'\\u1d25', u'\\u3153', u'\\u3150', u'\\u3134', u'\\u1d25', u'\\u3134', u'\\u314f', u'\\u1d25', u'\\u3134', u'\\u3146', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u3163', u'\\u3139', u'\\u1d25', u' ', u'\\u3157', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u1d25', u'\\u3146', u'\\u3141', u'\\u1d25', u'\\u3147', u'\\u3163', u'\\u3139', u'\\u1d25', u'\\u3131', u'\\u3161', u'\\u3131', u'\\u1d25', u'\\u1d25', u'\\u3146', u'\\u3161', u'\\u3134', u'\\u3161', u'\\u314f', u'\\u1d25', u' ', u'\\u3163', u'\\u3147', u'\\u3148', u' ', u'\\u3155', u'\\u315c', u'\\u1d25', u'\\u3139', u' ', u'\\u3146', u'\\n', u' ', u'\\u3153', u'\\u3136', u'\\u3147', u'\\u1d25', u'\\u3141', u'.', u'\\u3131', u'\\u314f', u'\\u1d25', u' ', u'\\u3137', u'\\u314f', u'\\u1d25', u'\\u3147', u'\\u314f', u'\\u1d25', u'\\u3141', u'\\u3137', u'\\u3131', u'\\u315c', u'\\u1d25', u'\\u1d25', u' ', u'.', u' ', u'\\u3161', u'\\u1d25', u'\\u1d25', u'.', u'\\u3153', u'\\u3163', u'\\u1d25', u'\\u3145', u'\\u3154', u'\\u1d25', u'\\u3132', u'\\u3147', u'\\u3146', u'\\u1d25', u' ', u'E', u'\\u3134', u'\\u3134', u'\\u1d25', u'\\u3147', u'\\u3134', u'\\u3134', u'\\u3145', u'\\u315b', u'\\u3163', u'\\u1d25', u'\\u1d25', u' ', u'\\u1d25', u'\\u3147', u'\\u1d25', u'\\u1d25', u'\\u3134', u'\\u3163', u'\\u1d25', u' ', u'\\u1d25', u'\\u3161', u'\\u3131', u'\\u1d25', u'\\u1d25', u'\\u3155', u'\\u3154', u'\\u1d25', u'\\u3145', u'\\u3157', u'\\u3153', u' ', u' ', u'\\u3139', u'\\u1d25', u' ', u'\\u3150', u'\\u3163', u'\\u1d25', u' ', u'\\u3137', u'\\u314f', u'\\u1d25', u'\\u3137', u'\\u3161', u'\\u3153', u'\\u1d25', u'\\u3147', u'\\u3137', u' ', u'\\u3137', u'\\u315f', u'\\u3157', u'\\u1d25', u'\\u1d25', u' ', u' ', u'\\u314f', u'\\u1d25', u'\\u3142', u'\\u3136', u'\\u1d25', u'\\u3139', u'\\u3161', u'\\u1d25', u'\\r', u'\\u1d25', u'\\u1d25', u'\\u314e', u'\\u3136', u'\\u3157', u'\\u1d25', u'\\u3147', u'\\u3146', u'\\u3141', u'\\u314e', u'\\u314f', u'\\u3155', u'\\u1d25', u'\\u314f', u'\\u3153', u'\\u1d25', u' ', u'\\u1d25', u'\\u3157', u'\\u3147', u'\\u3132', u'\\u3146', u'\\u1d25', u' ', u' ', u'\\u3141', u'\\u3163', u'\\u1d25', u' ', u'\\u314f', u'\\u1d25', u'.', u'\\u3161', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u3146', u'\\u1d25', u'\\u3137', u'\\u3146', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u3153', u'\\u1d25', u'\\u3161', u'\\u3134', u'\\u1d25', u'\\u3137', u'\\u3158', u'\\u1d25', u'\\u1d25', u'\\u315a', u'\\u1d25', u'\\u3145', u'\\u3147', u'\\u3155', u'\\u3134', u'\\u1d25', u'\\u3147', u'\\u314e', u'\\n', u' ', u'\\u3131', u'\\u3139', u'\\u1d25', u'\"', u'\\u3131', u'\\u3163', u'\\u1d25', u'\\u1d25', u'\\u3148', u'\\u1d25', u'\\u3134', u'\\u3134', u'\\u1d25', u'\\u3137', u'\\u1d25', u'\\u1d25', u'\\u3163', u'\\u1d25', u'\\u3131', u'\\u3131', u'\\u314f', u'\\u1d25', u'\\u3146', u'\\u314e', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u3153', u'\\u3154', u'\\u1d25', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u1d25', u'.', u'\\u3147', u'\\u3142', u'\\u3134', u'\\u1d25', u'\\r', u'\\u314f', u'\\u3134', u'\\u1d25', u'\\u3131', u'\\u1d25', u'\\u315a', u'\\u1d25', u'?', u'\\u3162', u'\\u1d25', u'\\u1d25', u' ', u'\\u3157', u'\\u1d25', u' ', u'\\u3157', u'\\u1d25', u'\\u3139', u'\\u3163', u'\\u3161', u' ', u'\\u3147', u'\\u1d25', u'\\u1d25', u' ', u'\\u1d25', u'\\u1d25', u'\\u3134', u'\\u1d25', u'\\u1d25', u' ', u'\\u3145', u'\\u314f', u'\\u1d25', u'\\u3139', u'\\u3157', u'\\u1d25', u'\\u1d25', u'\\u3163', u'\\n', u' ', u'\\u3161', u'\\u314f', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u3147', u'\\u3139', u'\\u1d25', u'\\u3131', u'\\u3134', u'\\u1d25', u'\\u3147', u'\\u1d25', u'\\u3141', u'\\u3131', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u315c', u'\\u1d25', u'\\u3148', u'\\u3146', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u3163', u'\\u1d25', u' ', u'\\u3146', u'\\u1d25', u'\\u1d25', u'\\u314e', u'\\u315c', u'\\u1d25', u'\\u1d25', u'\\u314f', u'\\u3163', u'\\u1d25', u'\\u1d25', u'\\u3161', u'?', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u3136', u'\\u3131', u' ', u'\\u3153', u'\\u3134', u'\\u3139', u'\\u1d25', u'\\u3147', u'\\u1d25', u'\\u3155', u'\\u1d25', u' ', u'\\u1d25', u'\\u3157', u'\\u1d25', u'\\u3155', u'\\u1d25', u'\\u3131', u'\\u3163', u'\\u1d25', u'.', u'\\u3148', u'\\u3137', u'\\u3154', u'\\u3134', u'\\u1d25', u'\\u3134', u'\\u3157', u'\\u3142', u'\\u3134', u'\\u3147', u'\\u3148', u'\\u1d25', u' ', u'\\u313c', u'\\u1d25', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u3161', u',', u' ', u'\\u314f', u'\\u314f', u'\\u3161', u' ', u' ', u'\\u3141', u'\\u1d25', u' ', u'\\u1d25', u'\\u3147', u'\\u1d25', u'\\u3134', u'\\u3134', u'\\u1d25', u' ', u'\\u3154', u'\\u1d25', u'\\u1d25', u'\\u3141', u'\\u314f', u'\\u3139', u'\\u1d25', u' ', u' ', u'\\u315c', u'\\u314f', u'\\u3146', u'\\u3145', u'\\u1d25', u'\\u314c', u'\\u314f', u'\\u1d25', u'\\u1d25', u'.', u'\\u314f', u'\\u1d25', u' ', u' ', u'\\u3147', u'\\u3158', u'\\u1d25', u' ', u'\\u3162', u'\\u1d25', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u3146', u'\\u1d25', u'\\u3157', u'\\u3163', u'\\n', u' ', u'\\u3147', u'\\u3151', u'\\u1d25', u' ', u' ', u'\\u3153', u' ', u' ', u'\\u3134', u'\\u3147', u'\\u3161', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u3131', u'\\u3153', u'\\u1d25', u'\\u1d25', u' ', u'\\u3162', u'\\u1d25', u'\\u3148', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u3148', u'\\u3131', u'\\u1d25', u'\\u3131', u'\\u3163', u'\\u3157', u'\\u1d25', u'\\u3150', u'\\u3161', u' ', u'\\u315c', u'\\u314f', u'\\u1d25', u' ', u'.', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u3131', u'\\u3161', u'\\u3131', u'\\u3157', u'\\u3139', u'\\u1d25', u'\\u3147', u'\\u3137', u'\\u314f', u'\\u1d25', u'\\u314f', u'\\u3147', u'\\u1d25', u'\\u3141', u'\\u3161', u'\\u1d25', u' ', u'\\u3163', u'\\u1d25', u'\\u1d25', u'\\u3145', u'\\u3131', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u3146', u'\\n', u'\\u3139', u'\\u3151', u'\\u1d25', u' ', u'\\u1d25', u'\\u315c', u'\\u3139', u'\\u3145', u' ', u'\\u1d25', u'\\u3153', u'\\u1d25', u'.', u'\\u3147', u'\\u3153', u'\\u1d25', u'\\u1d25', u'\\u3163', u'\\u3146', u'\\u3134', u'\\u1d25', u' ', u'\\u3157', u'\\u1d25', u'\\u3163', u'\\u3146', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u3145', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u3131', u'\\u3157', u'\\u1d25', u'\\u1d25', u'\\u3157', u'\\u1d25', u' ', u'\\u3161', u'\\u314f', u'\\u1d25', u'\\u1d25', u' ', u'\\u3147', u'\\u3157', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u3147', u'\\u3161', u'\\n', u'\\u3134', u'\\u314f', u'\\u1d25', u'\\u3161', u'\\u314e', u'\\u3142', u'\\u3137', u'\\u315f', u'\\u1d25', u'\\u1d25', u'\\u3148', u'\\n', u'\\u3134', u'\\u3153', u'\\u1d25', u'\\u3147', u\"'\", u'\\u1d25', u'\\u3141', u'\\u315c', u'\\u1d25', u'\\u3132', u'\\u3153', u'\\u1d25', u'\\u1d25', u'\\u314f', u'\\u1d25', u'\\u3131', u'\\u3137', u'\\u3139', u'\\u1d25', u'\\u3134', u'\\u1d25', u'\\u3139', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u314f', u'\\u3141', u'\\u3161', u'\\u315c', u'\\n', u'\\u1d25', u'\\u3155', u'\\u3148', u'\\u3157', u'\\u1d25', u'\\u314a', u'\\u314f', u'\\u1d25', u' ', u'\\u3137', u'\\u3161', u'\\n', u'\\u1d25', u'\\u3163', u'\\u1d25', u' ', u'\\u3144', u'\\u3134', u'\\u1d25', u'\\u3137', u'\\u314f', u'\\u3153', u'.', u'\"', u'\\u3153', u'\\u3155', u'\\u1d25', u'\\u1d25', u'\\r', u'\\u3137', u'.', u'\\u3131', u'\\u3145', u'\\u1d25', u'\\u3134', u' ', u'\\u1d25', u'\\u3131', u'\\u1d25', u'\\u1d25', u'\\u3142', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u314f', u'\\u3134', u'\\u1d25', u' ', u'\\u3137', u'\\u314f', u'\\u1d25', u' ', u'\\u1d25', u'\\u3153', u'\\u3161', u'\\u1d25', u' ', u'.', u'\\u3145', u'\\u1d25', u'\\u1d25', u' ', u'\\u3134', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u3153', u'\\u1d25', u' ', u'\\u3147', u'\\u3131', u'\\u314f', u'\\u1d25', u'\\u3134', u'\\u3147', u'\\u1d25', u'\\u1d25', u'\\u1d25', u' ', u' ', u'\\u3131', u'\\u3147', u'\\u3155', u'\\u1d25', u' ', u'\\r', u'\\u1d25', u'\\u1d25', u'\\u315c', u'\\u3153', u'\\u1d25', u'\\u3161', u'\\u3141', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u3147', u'\\u3157', u'\\u3141', u' ', u'\\u3141', u'\\u1d25', u'\\u1d25', u'\\u3142', u'\\u3134', u'\\u3134', u'\\u3134', u'\\u3134', u'\\u1d25', u' ', u'\\u315c', u'\\u1d25', u'\\u3141', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u3142', u'\\u3137', u'\\u3141', u'\\u3148', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u314e', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u3153', u'\\u3161', u'\\u3134', u'\\u1d25', u'\\u315c', u'\\u3163', u'\\u3134', u'\\u1d25', u'\\u1d25', u' ', u'\\u3145', u'\\u3157', u'\\u1d25', u' ', u'\\u314f', u'\\u3146', u'\\u1d25', u'\\r', u'\\u314f', u'\\u3161', u'\\u1d25', u'\\u1d25', u'\\u314f', u'\\u3141', u'\\u1d25', u' ', u'\\u3137', u'\\u315c', u'\\u1d25', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u315c', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u3163', u'\\u1d25', u'\\u1d25', u'\\u3153', u'\\u1d25', u'\\u3147', u' ', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u3131', u'\\u314f', u'\\u1d25', u' ', u'\\u1d25', u'\\u3141', u'\\u3142', u' ', u'\\u1d25', u'\\u3163', u'\\u1d25', u'\\u1d25', u'\\u3139', u'\\u3155', u'\\u1d25', u'\\u3141', u' ', u'\\u3134', u'\\u1d25', u' ', u'\\u3157', u'\\u1d25', u'\\r', u'\\u1d25', u'.', u'\\u3134', u'\\u3157', u'\\u1d25', u'\\u3137', u'\\u3157', u'\\u1d25', u'\\u3137', u'\\u3155', u'\\u3154', u'\\u1d25', u' ', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u3142', u'\\u3163', u'\\u1d25', u'\\u3147', u'\\u1d25', u'\\u3147', u'\\u3134', u'\\u1d25', u' ', u'\\u3137', u'\\u315c', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u3154', u'\\u1d25', u'\\u315a', u'\\u3131', u'\\u1d25', u'\\u3142', u'\\u3141', u'\\u1d25', u'\\u3147', u'\\n', u'\\u3141', u'\\u3161', u'\\u1d25', u'\\u3139', u'\\u315c', u'\\u1d25', u' ', u'\\u3153', u'\\u3131', u'\\u1d25', u'\\u1d25', u' ', u'\\u1d25', u'\\u3161', u'\\u3146', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u3147', u'\\u3153', u'\\u3134', u'\\u1d25', u'\\u3131', u' ', u'\\u1d25', u'\\u1d25', u'\\u3147', u'\\u3163', u'\\u1d25', u'\\u3162', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u3163', u'\\u315c', u'\\u1d25', u'\\u3134', u'\\u3161', u'\\u3157', u'\\u3147', u' ', u'\\u315b', u'\\u3163', u'\\u1d25', u' ', u' ', u'\\u1d25', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u3139', u'\\u3150', u'\\n', u'\\u314f', u'\\u314f', u'.', u'\\u3131', u' ', u'\\u3134', u'\\u3134', u'\\u3131', u'\\u1d25', u'\\u314a', u'\\u1d25', u'\\u1d25', u'\\u3145', u'\\u3161', u'\\u314f', u'\\u3131', u'\\u314e', u'\\u3161', u'\\u3134', u'\\u1d25', u'\\u1d25', u'\\u315b', u'\\u1d25', u'\\u3134', u'\\u3151', u'\\u1d25', u' ', u'\\u315c', u'\\u3139', u'\\u315c', u'\\u1d25', u',', u'\\u3150', u'\\u1d25', u'\\u3139', u'\\u3159', u'\\u3139', u'\\u1d25', u'\\u1d25', u'\\u3141', u'\\u3157', u'\\u1d25', u' ', u'\\u3147', u'\\n', u'\\u1d25', u'\\u314f', u'\\u1d25', u'\\u3147', u'\\u3131', u'\\n', u' ', u' ', u'\\u3145', u'\\u314f', u'\\u1d25', u'\\u3137', u'\\u3131', u'\\n', u'\\u1d25', u'\\u1d25', u'\\u314f', u'\\u1d25', u'\\u1d25', u'\\u1d25', u'\\u3134', u'\\u1d25', u'\\u3131', u'\\u3163', u'\\u1d25', u'\\u1d25']\n",
      "\n",
      "Composed Text : 오늘 ㅈㄱㄷㅏㅂㅇㅡㅔ뽜ㅏㄱㄹ너ㅓㅐㄴ나ㄴㅆㄹㅣㄹ ㅗㅇㅆㅁ일극쓴ㅡㅏ ㅣㅇㅈ ㅕㅜㄹ ㅆ\n",
      "ㅎㄶㅗㅇㅆㅁ하ㅕㅏㅓ ㅗㅇㄲㅆ  미 ㅏ.ㅡㄴㅆㄷㅆㅓㅡㄴ돠ㅚㅅ연ㅇㅎ\n",
      "ㅏㄴㄱㅚ?ㅢ ㅗ ㅗ리ㅡ ㅇ ㄴ 사로ㅣ\n",
      " ㅡㅏㄹㅇㄹㄱㄴㅇㅁㄱㄴㅜㅈㅆ이 ㅆ후ㅏㅣㅡ?ㅇㄶㄱ ㅓㄴㄹㅇㅕ ㅗㅕ기.ㅈ덴놉ㄴㅇㅈ ㄼㄴㅡ, ㅏㅏㅡ  ㅁ ㅇㄴㄴ ㅔ말  ㅜㅏㅆㅅ타.ㅏ  와 ㅢㄹㅆㅗㅣ\n",
      " 야  ㅓ  ㄴ을거 ㅢㅈㄴㅈㄱ기ㅗㅐㅡ ㅜㅏ .ㅏ극ㅗㄹㅇ다ㅏㅇ므 ㅣㅅ가ㅆ\n",
      "랴 ㅜㄹㅅ ㅓ.어ㅣㅆㄴ ㅗㅣㅆㄹ사고ㅗ ㅡㅏ 오ㄹ으\n",
      "나ㅡㅎㅂ뒤ㅈ\n",
      "너ㅇ'무꺼ㅏㄱㄷㄹㄴ라ㅏ므ㅜ\n",
      "ㅕ조차 드\n",
      ".노도뎌ㅔ ㄹㅇ비ㅇㅇㄴ 두ㅔㅚㄱㅂㅁㅇ\n",
      "므루 ㅓㄱ ㅡㅆㅇ언ㄱ 이ㅢㄹㅣㅜ느ㅗㅇ ㅛㅣ  ㅏㅏ래\n",
      "ㅏㅏ.ㄱ ㄴㄴㄱㅊ스ㅏㄱ흔ㅛ냐 ㅜ루,ㅐ뢜모 ㅇ\n",
      "ㅏㅇㄱ\n",
      "  사ㄷㄱ\n",
      "ㅏㄴ기\n"
     ]
    }
   ],
   "source": [
    "# Sample ! \n",
    "save_dir = \"save\"\n",
    "prime = decompose_text(u\"오늘 \")\n",
    "\n",
    "print (\"Prime Text : %s => %s\" % (automata(prime), \"\".join(prime)))\n",
    "n = 1000\n",
    "with open(os.path.join(save_dir, 'config.pkl'), 'rb') as f:\n",
    "    saved_args = cPickle.load(f)\n",
    "with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "    chars, vocab = cPickle.load(f)\n",
    "\n",
    "print chars\n",
    "print vocab\n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "\n",
    "print (ckpt.model_checkpoint_path)\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sampled_text = sample(sess, chars, vocab, probs, n, prime)\n",
    "    print (\"\")\n",
    "    print (\"SAMPLED TEXT = %s\" % sampled_text)\n",
    "\n",
    "    print (\"\")\n",
    "    print (\"Composed Text : %s\" % automata(\"\".join(sampled_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
